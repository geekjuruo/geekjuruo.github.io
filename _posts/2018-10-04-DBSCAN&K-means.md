---
layout:     post                    # 使用的布局（不需要改）
title:    DBSCAN和K-means算法  	   # 标题 
subtitle:   	DBSCAN&K-means				# 副标题
date:       2018-10-04              # 时间
author:     LYH                      # 作者
header-img: img/post-sample-image.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 网络
    
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# DBSCAN算法和K-means算法
## DBSCAN
> Density-based spatial clustering of applications with noise,这个算法以密度为本：给定某空间里一个点集合，这算法能把附近的点分成一组，并标记出位于低密度区域的局外点（最接近它的点也十分遥远）

### 基础知识
为了进行DBSCAN聚类，所有的点将被分为核心点，（密度）可达点及局外点，详情如下：  

* 如果一个点 p 在距离 ε 范围内有至少 minPts 个点(包括自己)，则这个点被称为核心点，那些 ε 范围内的则被称为由 p 直接可达的。同时定义，没有任何点是由非核心点直接可达的。
* 如果存在一条道路 p1, ..., pn ，有 p1 = p和pn = q， 且每个 pi+1 都是由 pi 直接可达的(道路上除了 q 以外所有点都一定是核心点)，则称 q 是由 p 可达的。
* 所有不由任何点可达的点都被称为局外点。  

**如果p是核心点，则它与所有由它可达的点（包括核心点和非核心点）形成一个聚类，每个聚类拥有至少一个核心点，非核心点也可以是聚类的一部分。**

### 算法和复杂度
DBSCAN 需要两个参数：**ε (eps) 和形成高密度区域所需要的最少点数 (minPts)**，它由一个任意未被访问的点开始，然后探索这个点的 ε-邻域，如果 ε-邻域里有足够的点，则建立一个新的聚类，否则这个点被标签为杂音。注意这个点之后可能被发现在其它点的 ε-邻域里，而该 ε-邻域可能有足够的点，届时这个点会被加入该聚类中。  
如果一个点位于一个聚类的密集区域里，它的 ε-邻域里的点也属于该聚类，当这些新的点被加进聚类后，如果它(们)也在密集区域里，它(们)的 ε-邻域里的点也会被加进聚类里。这个过程将一直重复，直至不能再加进更多的点为止，这样，一个密度连结的聚类被完整地找出来。然后，一个未曾被访问的点将被探索，从而发现一个新的聚类或杂音。  
DBSCAN 对数据库里的每一点进行访问，可能多于一次（例如作为不同聚类的候选者），但在现实的考虑中，时间复杂度主要受regionQuery 的调用次数影响，DBSCAN 对每点都进行刚好一次调用，且如果使用了特别的编号结构，则总平均时间复杂度为 O(n log n) ，最差时间复杂度则为 O(n^2) 。可以使用 O(n^2) 空间复杂度的距离矩阵以避免重复计算距离，但若不使用距离矩阵，DBSCAN 的空间复杂度为 **O(n)**。

### 优缺点
#### 优点
1. 相比K-means算法，DBSCAN不需要预先声明聚类数量。
2. DBSCAN可以找出任何形状的聚类，甚至能找出一个聚类，它包围但不连接一个聚类，由于有MinPts参数的限制，single-link能被有效避免。
3. DBSCAN能分辨噪音，且只有两个参数。   

#### 缺点
1. DBSCAN 不是完全决定性的：在两个聚类交界边缘的点会视乎它在数据库的次序决定加入哪个聚类。  
2. DBSCAN 聚类分析的质素受函数 regionQuery(P,ε) 里所使用的度量影响，最常用的度量是欧几里得距离，尤其在高维度资料中，由于受所谓“维数灾难”影响，很难找出一个合适的 ε ，但事实上所有使用欧几里得距离的算法都受维数灾难影响。
3. 如果数据库里的点有不同的密度，而该差异很大，DBSCAN 将不能提供一个好的聚类结果，因为不能选择一个适用于所有聚类的 minPts-ε 参数组合。
\documentclass[a4paper,11pt]{article}
\usepackage{CJK,CJKnumb,CJKulem}
\usepackage{amsmath,amsthm,amsfonts,amssymb,bm}

## K-means
> k-means clustering,源于信号处理中的一种向量量化方法，现在则更多地作为一种聚类分析方法流行于数据挖掘领域。k-means聚类的目的是：把 n个点（可以是样本的一次观察或一个实例）划分到k个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。这个问题将归结为一个把数据空间划分为Voronoi cells的问题。

### 基础知识
已知观测集 (x_1,x_2,...,x_n)，其中每个观测都是一个d-维实向量，k-means聚类要把这n个观测划分到k个集合中(k≤n),使得组内平方和（WCSS within-cluster sum of squares）最小。换句话说，它的目标是找到使得下式满足的聚类S_i:  
$$\arg\underset{S}{\min}\sum_{i=1}^k\sum\_{x \in S\_{i}}||x-\mu\_{i}||^2$$

其中$$\mu\_{i}$$是$$S_i$$中所有点的均值。

### 算法和复杂度
最常用的算法是迭代优化的技术，有时候也被称为Lloyd算法。算法大致分为两个步骤交替进行：

* 分配（Assignment）：将每一个观测分配到聚类中，使得组内平方和（WCSS）达到最小。
* 更新（Update）： 对于上一步得到的每一个聚类，以聚类中心观测值的图心作为新的均值点，进一步减小目标函数组内平方和（WCSS）的值。

> 这一算法将在对于观测的分配不再变化时收敛，但无法保证得到全局最优解。

**在d维空间找到k-均值聚类问题的最优解的计算复杂度**  
Lloyds算法在实践中通常被认为几乎时线性复杂度的。